ipak <- function(pkg){
  new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
  if (length(new.pkg)) 
    install.packages(new.pkg, dependencies = TRUE)
  sapply(pkg, require, character.only = TRUE)
}

packages <- c("see","jtools","olsrr","parameters","stats","ggplot2", "palmerpenguins", "plot3D" , "plot3Drgl","apaTables","gvlma","broomExtra","performance")
ipak(packages)

#cambiar a vuestro directorio de trabajo.
setwd('C:/Users/pable/Desktop/Doctorado')

data(package = 'palmerpenguins')
head(penguins)
df<-as.data.frame(penguins)
df<-df[complete.cases(df), ]
scatter.smooth(x=df$flipper_length_mm, y=df$body_mass_g, main="masa ~ aleta")

#regresión lineal simple

linearMod <- lm(body_mass_g ~ flipper_length_mm, data=df)  # build linear regression model on full data

#comparar los supuestos
#interpretación en https://cran.r-project.org/web/packages/gvlma/gvlma.pdf pag 7

gvlma(linearMod)
summary(gvlma(linearMod))
plot(gvlma(linearMod))

#vemos la regresion
print(linearMod)
summary(linearMod)

#otra opción. paquete parameters

model_parameters(linearMod, bootstrap = TRUE, iterations = 200)

#intervalos de confianza
confint(linearMod)

#otra opción aunque sin sentido para la regresión simple.
#paquete olsrr
ols_step_both_p(linearMod)

#masa = Intercept + (?? ???aleta)
#masa = -5872.09 + 50.15*aleta


#For model comparison, the model with the lowest AIC and BIC score is preferred.
AIC(linearMod)  # AIC => 
BIC(linearMod)  # BIC =>

#modelo de predicción
library(readxl)
testData <- read_excel("testData.xlsx")
View(testData)

pesoPred <- predict(linearMod, testData)
actuals_preds <- data.frame(cbind(actuals=testData$Peso, predicteds=pesoPred))  # make actuals_predicteds dataframe.
correlation_accuracy <- cor(actuals_preds)  
correlation_accuracy
head(actuals_preds)
scatter.smooth(x=df$flipper_length_mm, y=df$body_mass_g, main="masa ~ aleta")
sd(df$body_mass_g)

#visualizaciones

ggstatsplot::ggcoefstats(
  x = stats::lm(body_mass_g ~ flipper_length_mm, data=df),
  sort = "ascending", # sorting the terms of the model based on estimate values
  ggtheme = ggplot2::theme_gray(), # changing the default theme
  stats.label.color = c("#CC79A7"),
  title = "Chupito R",
  subtitle = "Suscríbete"
) +
  # further modification with the ggplot2 commands
  # note the order in which the labels are entered
  ggplot2::scale_y_discrete(labels = c(
    "aleta")) +
  ggplot2::labs(y = "etiqueta y") +
  ggplot2::theme(axis.title.y = ggplot2::element_text(size = 14, face = "bold"))

#otra opción paq:  jtools

linearMod1 <- lm(body_mass_g ~ flipper_length_mm, data=df)
linearMod2 <- lm(body_mass_g ~ bill_depth_mm, data=df)
plot_summs(linearMod1, linearMod2, scale = TRUE)
#Plot coefficient uncertainty as normal distributions
plot_summs(linearMod1, scale = TRUE, plot.distributions = TRUE, inner_ci_level = .9)



#ultimo antes de pasar a las gráficas de dispersión.
#paquete performance

m1 <- lm(body_mass_g ~ flipper_length_mm, data=df)
m2 <- lm(body_mass_g ~ bill_depth_mm, data=df)
m3 <- lm(body_mass_g ~ bill_length_mm, data=df)

compare_performance(m1, m2, m3, rank = TRUE)
plot(compare_performance(m1, m2, m3, rank = TRUE))

#graficar
ggplot(df, aes(x=flipper_length_mm, y=body_mass_g, color=sex, shape=sex)) +
  geom_point()+ 
  geom_smooth(method=lm, aes(fill=sex))+
  geom_density_2d()

#función vwReg   #Felix Schönbrodt's blog

#https://rdrr.io/github/vinash85/avinash/src/R/vwReg.R

vwReg <- function(formula, data, title="", B=1000, shade=TRUE, shade.alpha=.1, spag=FALSE, spag.color="darkblue", mweight=TRUE, show.lm=FALSE, show.median = TRUE, median.col = "white", shape = 21, show.CI=FALSE, method=loess, bw=FALSE, slices=200, palette=colorRampPalette(c("#FFEDA0", "#DD0000"), bias=2)(20), ylim=NULL, quantize = "continuous",  add=FALSE, ...) {
  IV <- all.vars(formula)[2]
  DV <- all.vars(formula)[1]
  data <- na.omit(data[order(data[, IV]), c(IV, DV)])
  
  if (bw == TRUE) {
    palette <- colorRampPalette(c("#EEEEEE", "#999999", "#333333"), bias=2)(20)
  }
  
  print("Computing boostrapped smoothers ...")
  newx <- data.frame(seq(min(data[, IV]), max(data[, IV]), length=slices))
  colnames(newx) <- IV
  l0.boot <- matrix(NA, nrow=nrow(newx), ncol=B)
  
  l0 <- method(formula, data)
  for (i in 1:B) {
    data2 <- data[sample(nrow(data), replace=TRUE), ]
    data2 <- data2[order(data2[, IV]), ]
    if (class(l0)=="loess") {
      m1 <- method(formula, data2, control = loess.control(surface = "i", statistics="a", trace.hat="a"), ...)
    } else {
      m1 <- method(formula, data2, ...)
    }
    l0.boot[, i] <- predict(m1, newdata=newx)
  }
  
  # compute median and CI limits of bootstrap
  library(plyr)
  library(reshape2)
  CI.boot <- adply(l0.boot, 1, function(x) quantile(x, prob=c(.025, .5, .975, pnorm(c(-3, -2, -1, 0, 1, 2, 3))), na.rm=TRUE))[, -1]
  colnames(CI.boot)[1:10] <- c("LL", "M", "UL", paste0("SD", 1:7))
  CI.boot$x <- newx[, 1]
  CI.boot$width <- CI.boot$UL - CI.boot$LL
  
  # scale the CI width to the range 0 to 1 and flip it (bigger numbers = narrower CI)
  CI.boot$w2 <- (CI.boot$width - min(CI.boot$width))
  CI.boot$w3 <- 1-(CI.boot$w2/max(CI.boot$w2))
  
  
  # convert bootstrapped spaghettis to long format
  b2 <- melt(l0.boot)
  b2$x <- newx[,1]
  colnames(b2) <- c("index", "B", "value", "x")
  
  library(ggplot2)
  library(RColorBrewer)
  
  # Construct ggplot
  # All plot elements are constructed as a list, so they can be added to an existing ggplot
  
  # if add == FALSE: provide the basic ggplot object
  p0 <- ggplot(data, aes_string(x=IV, y=DV)) + theme_bw()
  
  # initialize elements with NULL (if they are defined, they are overwritten with something meaningful)
  gg.tiles <- gg.poly <- gg.spag <- gg.median <- gg.CI1 <- gg.CI2 <- gg.lm <- gg.points <- gg.title <- NULL
  
  if (shade == TRUE) {
    quantize <- match.arg(quantize, c("continuous", "SD"))
    if (quantize == "continuous") {
      print("Computing density estimates for each vertical cut ...")
      flush.console()
      
      if (is.null(ylim)) {
        min_value <- min(min(l0.boot, na.rm=TRUE), min(data[, DV], na.rm=TRUE))
        max_value <- max(max(l0.boot, na.rm=TRUE), max(data[, DV], na.rm=TRUE))
        ylim <- c(min_value, max_value)
      }
      
      # vertical cross-sectional density estimate
      d2 <- ddply(b2[, c("x", "value")], .(x), function(df) {
        res <- data.frame(density(df$value, na.rm=TRUE, n=slices, from=ylim[1], to=ylim[2])[c("x", "y")])
        #res <- data.frame(density(df$value, na.rm=TRUE, n=slices)[c("x", "y")])
        colnames(res) <- c("y", "dens")
        return(res)
      }, .progress="text")
      
      maxdens <- max(d2$dens)
      mindens <- min(d2$dens)
      d2$dens.scaled <- (d2$dens - mindens)/maxdens  
      
      ## Tile approach
      d2$alpha.factor <- d2$dens.scaled^shade.alpha
      gg.tiles <-  list(geom_tile(data=d2, aes(x=x, y=y, fill=dens.scaled, alpha=alpha.factor)), scale_fill_gradientn("dens.scaled", colours=palette), scale_alpha_continuous(range=c(0.001, 1)))
    }
    if (quantize == "SD") {
      ## Polygon approach
      
      SDs <- melt(CI.boot[, c("x", paste0("SD", 1:7))], id.vars="x")
      count <- 0
      d3 <- data.frame()
      col <- c(1,2,3,3,2,1)
      for (i in 1:6) {
        seg1 <- SDs[SDs$variable == paste0("SD", i), ]
        seg2 <- SDs[SDs$variable == paste0("SD", i+1), ]
        seg <- rbind(seg1, seg2[nrow(seg2):1, ])
        seg$group <- count
        seg$col <- col[i]
        count <- count + 1
        d3 <- rbind(d3, seg)
      }
      
      gg.poly <-  list(geom_polygon(data=d3, aes(x=x, y=value, color=NULL, fill=col, group=group)), scale_fill_gradientn("dens.scaled", colours=palette, values=seq(-1, 3, 1)))
    }
  }
  
  print("Build ggplot figure ...")
  flush.console()
  
  
  if (spag==TRUE) {
    gg.spag <-  geom_path(data=b2, aes(x=x, y=value, group=B), size=0.7, alpha=10/B, color=spag.color)
  }
  
  if (show.median == TRUE) {
    if (mweight == TRUE) {
      gg.median <-  geom_path(data=CI.boot, aes(x=x, y=M, alpha=w3^3), size=.6, linejoin="mitre", color=median.col)
    } else {
      gg.median <-  geom_path(data=CI.boot, aes(x=x, y=M), size = 0.6, linejoin="mitre", color=median.col)
    }
  }
  
  # Confidence limits
  if (show.CI == TRUE) {
    gg.CI1 <- geom_path(data=CI.boot, aes(x=x, y=UL), size=1, color="red")
    gg.CI2 <- geom_path(data=CI.boot, aes(x=x, y=LL), size=1, color="red")
  }
  
  # plain linear regression line
  if (show.lm==TRUE) {gg.lm <- geom_smooth(method="lm", color="darkgreen", se=FALSE)}
  
  gg.points <- geom_point(data=data, aes_string(x=IV, y=DV), size=1, shape=shape, fill="white", color="black")       
  
  if (title != "") {
    gg.title <- theme(title=title)
  }
  
  
  gg.elements <- list(gg.tiles, gg.poly, gg.spag, gg.median, gg.CI1, gg.CI2, gg.lm, gg.points, gg.title, theme(legend.position="none"))
  
  if (add == FALSE) {
    return(p0 + gg.elements)
  } else {
    return(gg.elements)
  }
}

vwReg(body_mass_g ~ flipper_length_mm,df)
vwReg(body_mass_g ~ flipper_length_mm,df,family="symmetric",show.CI=TRUE)
vwReg(body_mass_g ~ flipper_length_mm,df,spag = 0.3,show.median = TRUE)
display.brewer.all()
vwReg(body_mass_g ~ flipper_length_mm,df,palette = brewer.pal(9,"YlGnBu"))
vwReg(body_mass_g ~ flipper_length_mm,df,quantize = "SD")
vwReg(body_mass_g ~ flipper_length_mm,df,shade.alpha = 0,slices = 400, family="symmetric",
      palette = colorRampPalette(c("black","green","yellow","red"),bias = 5)
      (20))
